{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from itertools import repeat\n",
    "# import airflow\n",
    "import shutil\n",
    "import sqlite3\n",
    "from DBConnector import Save_User,readControlDataLake,Update,connect_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "\n",
    "Path = 'files'\n",
    "global DestinationPath\n",
    "DestinationPath = 'DataLake'\n",
    "# This function will create an ID to identify tables\n",
    "def UniqueID (TableName):\n",
    "    mybytes = TableName.encode('utf-8')\n",
    "    ID = int.from_bytes(mybytes, 'little')\n",
    "    return ID\n",
    "\n",
    "def Update (TableID,Table_Name,UploadDate,ModificationDate,Origin,Destiny): \n",
    "    TableIDstr = str(TableID)\n",
    "    db = connect_db()    \n",
    "    print(TableID,Table_Name,UploadDate,ModificationDate,Origin,Destiny)\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"INSERT INTO ControlDataLake(TableID,TableName,UploadDate,ModificationDate,Origin,Destiny) VALUES(?,?,?,?,?,?)\",(TableIDstr,Table_Name,UploadDate,ModificationDate,Origin,Destiny))\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Governed Table (Metadata Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ExtractMetadata (): #Function to extract the metadata in order to include in the governed table\n",
    "    metadata = dict()\n",
    "    arr = os.listdir(Path)\n",
    "    for csvFiles in arr:\n",
    "        FilePath = Path + '/' + csvFiles\n",
    "        # print (FilePath)\n",
    "        # print(os.stat(FilePath))\n",
    "        metadata.update({csvFiles:os.stat(FilePath)})\n",
    "    return metadata\n",
    "\n",
    "def ExtractFilenames (): #The function will extract all the names from the path origin \n",
    "    arr = os.listdir(Path)\n",
    "    FileNames = []\n",
    "    for csvFiles in arr:\n",
    "        FilePath = csvFiles #Path + '/' + csvFiles\n",
    "        FileNames.append(FilePath)\n",
    "    return FileNames\n",
    "\n",
    "def GovernedTable (): #This function will create a table to index and filter fields and information about data files\n",
    "    FileNames = ExtractFilenames()\n",
    "    metadata = ExtractMetadata()\n",
    "    DataGovern = pd.DataFrame ()\n",
    "    TableFields = pd.DataFrame ()\n",
    "    \n",
    "    \n",
    "    for names in FileNames:  #It will explore diferents files which are load\n",
    "        listacolumnas = dict()\n",
    "        Extension = names[len(names)-3 : len(names)]\n",
    "        \n",
    "        if Extension == 'csv': #file is a csv\n",
    "            df= pd.read_csv(Path + '/' + names)\n",
    "            \n",
    "            columns = list(df.columns)\n",
    "            types   = list(df.dtypes)\n",
    "            IDTables  =list (repeat(UniqueID(names),len(types)))\n",
    "            TableFieldsnew = pd.DataFrame ({'TableId':IDTables,'Column Names':columns,'Data Type':types })\n",
    "            TableFields = pd.concat([TableFields, TableFieldsnew])\n",
    "        else:\n",
    "                 IDTables =  UniqueID()\n",
    "\n",
    " \n",
    "        if Extension == 'csv' or Extension == 'xlsx':   \n",
    "            StructureData = 'Yes'\n",
    "        else: \n",
    "            StructureData = 'No'\n",
    "        # Data Governed table which inform about the metadata\n",
    "        DataGovernnew =  pd.DataFrame ({'TableId':IDTables[1],\n",
    "                                     'Table Name': names,\n",
    "                                     'Comment':'',  \n",
    "                                          'Creation Date': datetime.datetime.fromtimestamp(metadata[names][9]),\n",
    "                                          'Modification Date': datetime.datetime.fromtimestamp(metadata[names][8]),\n",
    "                                          'File Size': metadata[names][6], \n",
    "                                          'File Type': Extension,\n",
    "                                          'Structured Data':'Yes',\n",
    "                                          'Load': 'No',\n",
    "                                          'DateLoad': 'Never'}, index=[0])\n",
    "                                           \n",
    "        \n",
    "        DataGovern  = pd.concat([DataGovern, DataGovernnew])\n",
    "        \n",
    "    #Reset the index    \n",
    "    DataGovern.reset_index(drop= True, inplace = True) \n",
    "    TableFields.reset_index(drop= True, inplace = True) \n",
    "    display(TableFields) \n",
    "    display (DataGovern)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Files to DataLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied successfully.\n",
      "3669323166161444168241861682847098757601682877257321371570083651948753533521390287071642917106938263462485 Uber Movement - Travel Times Methodology.pdf 2022-10-26 13:06:31.236266 2022-10-26 13:06:31.236266 files/Uber Movement - Travel Times Methodology.pdf DataLake/Uber Movement - Travel Times Methodology.pdf\n",
      "Save it in the data lake\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def copycsvfile (TableName):\n",
    "    try:\n",
    "        shutil.copy(Path + '/' + TableName , DestinationPath + '/' + TableName)\n",
    "        print(\"File copied successfully.\")\n",
    "    except shutil.SameFileError:\n",
    "         print(\"Source and destination represents the same file.\")\n",
    "    # If there is any permission issue\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\") \n",
    "    # For other errors\n",
    "    except:\n",
    "        print(\"Error occurred while copying file.\")\n",
    "    Update (UniqueID(TableName),str(TableName),str(datetime.datetime.now()),str(datetime.datetime.now()),Path + '/' + TableName,DestinationPath + '/' + TableName )\n",
    "    return print(\"Save it in the data lake\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Select Records  and insert them into governed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the columns we want to Load\n",
    "#Data should be available by time \n",
    "\n",
    "def Filter():\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transaction create storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory():\n",
    "    try:\n",
    "        print(\"Creating directory\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def delete_directory():\n",
    "    try:\n",
    "        print(\"Creating directory\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def rename_directory():\n",
    "    try:\n",
    "        print(\"Creating directory\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "def upload_file_to_directory():\n",
    "    try:\n",
    "        print(\"Creating directory\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "def download_file_from_directory():\n",
    "    try:\n",
    "        print(\"Creating directory\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "def list_directory_contents():\n",
    "    try:\n",
    "        print(\"Creating directory\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write data into a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage in HDFS \n",
    "Storage in MongoDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from governed table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create airflow, pipeline for manage it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "db0d44cd6751c020e8caac2ea46dea2316fe2111e9c16a6e9e18c15acd0d9215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
